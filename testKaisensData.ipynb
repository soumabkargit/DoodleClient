{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testKaisensData.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSNg99uuEv3IT1FOC+UFVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumabkargit/DoodleClient/blob/master/testKaisensData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNYDUQIYsU6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install datefinder\n",
        "!pip install -U -q PyDrive\n",
        "!pip install nltk\n",
        "!pip install dateparser\n",
        "\n",
        "import re\n",
        "import datefinder\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import nltk\n",
        "import sys\n",
        "from dateparser.search import search_dates \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtKxZ_MD1I7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match1(string):\n",
        "  res = re.search('(ab*)+', string)\n",
        "  return res\n",
        "\n",
        "text = 'zertyabtgyhavghbnabAezrtycvfgvghvgZertyuCcfvghbnjTftgvybhnYvgbhnjDFDFGazA'\n",
        "print(match2(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSsrA-R25ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match2(string):\n",
        "  \"\"\"return array of sequence found\"\"\"\n",
        "  res = re.findall('[A-Z]{1}[a-z]+', string)\n",
        "  return res\n",
        "\n",
        "text = 'zertyabtgyhavghbnabAezrtycvfgvghvgZertyuCcfvghbnjTftgvybhnYvgbhnjDFDFGazA'\n",
        "print(match2(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdoE_dnM3AjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match3(string):\n",
        "  res = re.findall('a{1}.+b$', string)\n",
        "  return res\n",
        "\n",
        "text = 'zertyabtgyhavghbnabAezrtycvfgvghvgZertyuCcfvghbnjTftgvybhnYvgbhnjDFDFGazA'\n",
        "print(match2(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M27RTHW3d-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_spliter(string, delimiters):\n",
        "  return re.split(delimiters, string)\n",
        "\n",
        "text = 'mariam , joue; à ! la balle.'\n",
        "print(custom_spliter(text,', |; |!'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOmOA9EgH51g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authentification Google\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download du fichier\n",
        "id_1 = '1m2ZGOaDZrjyc0pf5wn0Zu0xZSvxbf4Eo'\n",
        "downloaded = drive.CreateFile({'id': id_1})\n",
        "downloaded.GetContentFile('English_adverbs.txt')\n",
        "\n",
        "# Download du fichier\n",
        "id_2 = '1iUV4ZcMtIw4Pm-IUrO6Nm7zLxh0g5tpX'\n",
        "downloaded = drive.CreateFile({'id': id_2})\n",
        "downloaded.GetContentFile('French_adverbs.txt')\n",
        "\n",
        "en_adverbs_file = open('English_adverbs.txt')\n",
        "en_adverbs = []\n",
        "for line in en_adverbs_file:\n",
        "    en_adverbs.append(line.strip().lower())\n",
        "\n",
        "fr_adverbs_file =  open('French_adverbs.txt')\n",
        "fr_adverbs = []\n",
        "for line in fr_adverbs_file:\n",
        "    fr_adverbs.append(line.strip().lower())\n",
        "\n",
        "def find_adverbs(language):\n",
        "\tif language == 'english' :\n",
        "\t\treturn en_adverbs\n",
        "\telif language == 'french' :\n",
        "\t\treturn fr_adverbs\n",
        "\n",
        "\n",
        "def get_adverbs_by_language(string, language):\n",
        "\tif (language != 'french' and language != 'english'):\n",
        "\t\tprint('Unsupported language')\n",
        "\t\tsys.exit()\n",
        "\ttokens = nltk.word_tokenize(string, language =language)\n",
        "\tadverbs_ = find_adverbs(language)\n",
        "\tdict_adverbs = {}\n",
        "\tfor index in range(len(tokens)):\n",
        "\t\ttoken = tokens[index]\n",
        "\t\tif token in adverbs_:\n",
        "\t\t\tdict_adverbs[token]=(index + 1)\n",
        "\treturn dict_adverbs\n",
        "\n",
        "text = 'mariam joue souvent à la balle académiquement. abusivement'\n",
        "get_adverbs_by_language(text, 'french')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2g-d2x03j7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c453bfb6-05a1-4415-9da0-36cd83119caf"
      },
      "source": [
        "def remove_stop_words(string):\n",
        "  text_tokens = nltk.word_tokenize(string)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  return ' '.join(tokens_without_sw)\n",
        "\n",
        "\n",
        "def find_all_dates(string):\n",
        "  string = remove_stop_words(string)\n",
        "  print(string)\n",
        "  res = search_dates(string, languages=['fr', 'en'])\n",
        "  return res #[date[0] for date in res ]\n",
        "\n",
        "text = '12 janvier 1991 ce futm ma naissshjhnjk mais 12-01-2028 et 12 février 1988'\n",
        "print(find_all_dates(text))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 janvier 1991 futm naissshjhnjk 12-01-2028 12 février 1988\n",
            "[('12 janvier 1991', datetime.datetime(1991, 1, 12, 0, 0)), ('12-01-2028', datetime.datetime(2028, 12, 1, 0, 0)), ('février', datetime.datetime(2028, 2, 1, 0, 0)), ('1988', datetime.datetime(1988, 2, 1, 0, 0))]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}